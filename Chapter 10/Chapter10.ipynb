{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter  10 \n",
        "\n",
        "**1.  Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (ie. a single layer of Linear Threshold Units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?**\n",
        "\n",
        "A classical perceptron will only converge if the data is linearly seperable. It also cannot compute class probabilities. The logistic regression classifier is able to converge on non-linear data and outputs class probabilities.\n",
        "\n",
        "you could transfer it by change activation function to  sigmoid\n",
        "\n",
        "\n",
        "**2.Why was the logistic activation function a key ingredient in training the first MLPs**\n",
        "\n",
        "the activation function is smooth on all points which make gradient descent can't be zeros not as step fuction \n",
        "\n",
        "\n",
        "**3.Name three popular activation functions. Can you draw them?**\n",
        "1. tanch\n",
        "2. sigmoid\n",
        "3. Softmax\n",
        "4. Relu\n",
        "\n",
        "[Plots](https://www.kaggle.com/mahmoudreda55/activation-function)\n",
        "\n",
        "**4.. Suppose you have an MLP composed of one input layer with 10 passthrough\n",
        "neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activa‐tion function.**\n",
        "\n",
        "input layer >> (batch_size,10)\n",
        "\n",
        "hidden layer >> (10,50)\n",
        "\n",
        "output layer >> (50,3)\n",
        "\n",
        "shape of y >> (batch_size,3)\n",
        "\n",
        "y =  (W1 x + b1) * (W2 +b2)\n",
        "\n",
        "**5. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the out‐put layer, using what activation function? Answer the same questions for getting your network to predict housing prices as in**\n",
        "\n",
        "would you need 2 neurons in output layer with sigmoid or softmax as activation function \n",
        "\n",
        "would use 10  neurons in output layer with softmax as activation function \n",
        "\n",
        "**6.What is backpropagation and how does it work? What is the difference between\n",
        "backpropagation and reverse-mode autodiff?**\n",
        "\n",
        "Backpropagation is an algorithm used to train neural networks. It first computes the gradients of the cost function with regards to every model parameter then it performs a gradient descent step using these gradients.\n",
        "\n",
        "This backpropagation step is performed until the model parameters converge to values that hopefully minimize the cost function.\n",
        "\n",
        "Backpropagation refers to the whole process of training a neural network. Reverse-mode autodiff is a technique to compute the gradients efficiently. It is used by the backprop algorithm.\n",
        "\n",
        "**7.Can you list all the hyperparameters you can tweak in an MLP? If the MLP over‐fits the training data, how could you tweak these hyperparameters to try to solve the problem**\n",
        "\n",
        "learning rante -  N.hidden layer - N-of neurons in hidden layer - type of optimizer  -  type of activation function - batch size - epochs\n",
        "\n",
        "we can  use different method by working on try and see the result  or adding early stopping to the model  or any other regluzation method like dropout and L1 ,L2\n",
        "\n",
        "**8.. Train a deep MLP on the MNIST dataset and see if you can get over 98% preci‐sion. Try adding all the bells and whistles (i.e., save checkpoints, use early stop‐ping, plot learning curves using TensorBoard, and so on)**\n",
        "\n",
        "[Here](https://github.com/MAHMOUDRR707/Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow-Concept-s-Tools-and-Techniques/blob/master/Chapter%203/Mnist.ipynb)"
      ],
      "metadata": {
        "id": "EiesO-wj99jn"
      }
    }
  ]
}