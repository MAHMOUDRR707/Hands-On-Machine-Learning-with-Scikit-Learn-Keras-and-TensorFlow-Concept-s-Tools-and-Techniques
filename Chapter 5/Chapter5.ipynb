{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwNv7QF7S3fi"
      },
      "source": [
        "# 1. What is the fundamental idea behind Support Vector Machines?\n",
        "\n",
        "searches for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few instances may end up on the street). Another key idea is to use kernels when training on nonlinear datasets\n",
        "\n",
        "# 2. What is a support vector?\n",
        "\n",
        "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n",
        "\n",
        "\n",
        "# 3. Why is it important to scale the inputs when using SVMs? \n",
        "\n",
        " to limit the range of support vector (points )  so we could sperate between them \n",
        "\n",
        " \n",
        "# 4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?\n",
        "\n",
        "An SVM classifier can output the distance between the test instance and the decision boundary, and you can use this as a confidence score. However, this score cannot be directly converted into an estimation of the class probability. If you set probability=True when creating an SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM's scores. This will add the predict_proba() and predict_log_proba() methods to the SVM.\n",
        "\n",
        "# 5. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?\n",
        "\n",
        "\n",
        "This question applies only to linear SVMs since kernelized can only use the dual\n",
        "form. The computational complexity of the primal form of the SVM problem is proportional to the number of training instances \n",
        "\n",
        "\n",
        "# 6. Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should you increase or decrease Î³ (gamma)? What about C?\n",
        "\n",
        "There might be too much regularization. To decrease it, you need to increase gamma or C (or both).\n",
        "\n",
        "# 7. How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?\n",
        "\n",
        " We set the QP parameters for the hard margin problem as H, f, A, and b. The QP parameters for the soft margin problem will have m additional parameters and m additional constrains.\n",
        "\n",
        "Thus, H is equal to H plus m columns of 0s on the right and m rows of 0s at the bottom. f is euqal to f with m additional elements all equal to the value of hyperparameter C. A is equal to A with extra m*m identity matrix appended to the right. b is equal to b with m additional elements all equal to 0."
      ]
    }
  ]
}